\documentclass[12pt]{article}

\title{A Brief Report on the Problem of Least Squares Fitting in MATLAB.}
\author{Greg Ward}
\date{25 August, 1993}

\begin{document}

\maketitle

The problem of least-squares fitting in MATLAB first arose in trying
to perform delay correction on plasma data for RCBF studies.  The
delay correction involves fitting an equation in four parameters
($\alpha$, $\beta$, $\gamma$, and $\delta$) to the average brain
activity across gray matter, $A(t)$:
\begin{equation}
A(t) = \alpha g(t+\delta) \otimes e^{-\beta t} + \gamma g(t+\delta)
\label{eq:fiteqn}
\end{equation}
where $g(t+\delta)$ is the dispersion-corrected plasma activity
shifted by the delay time $\delta$.  Since this problem was found to
be overdetermined, we switched to the method recommended by Dr. Meyer
and used by Dr. Fujita's program: step through several values of
$\delta$, performing a three-parameter fit with $\alpha$, $\beta$, and
$\gamma$, and choose from among the values of $\delta$ that which give
the best ($\alpha, \beta, \gamma$) fit.

The trouble, of course, is that if {\em one} fit is rather slow (and
this is the case), then fifteen fits will be {\em very} slow.  There
are two possible approaches to curing this speed problem: at the
lowest level, one can attempt to speed up evaluation of the function
to be fitted; or, at the highest level, one can attempt to speed up
the whole fitting algorithm.

We initially chose the first alternative, to speed up \verb|b_curve.m|
by rewriting in C various routines which it calls.  (\verb|b_curve.m|
is a MATLAB function that, given $\alpha$, $\beta$, $\gamma$,
$g(t+\delta)$, the time-scale in which the plasma activity was
resampled, and the frame start times and lengths, calculates the
right-hand-side of Eq. \ref{eq:fiteqn}.)

The most critical of these routines was frameint, which performs the
task of integrating a set of data across several frames and dividing
by the frame lengths.  (In previous implementations of the RCBF
analysis, this was not done; rather, the data was simply interpolated
at the mid-frame time.  This can give similar values to the frame-wise
integration, assuming that the data is quite smooth.  It is also much
faster, but at the cost of mathematical accuracy.)  Upon re-writing
frameint in C, we observed a speed increase of roughly 50-fold, on the
order of the increase observed for other MATLAB-to-C translations.
This resulted in a speed increase of 2.5 times for
\verb|b_curve|.  (Specifically: 100 calls to \verb|b_curve| using the
MATLAB frameint took \verb|~|16.8 sec; the same trial with
\verb|b_curve| calling the CMEX routine nframeint took \verb|~|6.6 sec.
Tests were done on a 50 MHz SGI Indigo (ursula.mni.mcgill.ca) with no
other processor intensive processes running.  I also verified that the
results of the two versions of
\verb|b_curve| were identical.)

The next alternative is to rewrite the entirety of \verb|b_curve| in
C, rather than just bits and pieces of it.  This has been done (in
\verb|~greg/src/numanal/b_curve.c|), and it has been verified that the
results are within machine precision of the MATLAB implementation, but
we have not yet written a CMEX interface to test it from within MATLAB.

The other possible route to take is to replace the MATLAB minimisation
routine which we're using with a C equivalent.  This is desirable for
several reasons.

First, MATLAB comes with several flavours of minimisation/fitting
algorithms.  We have found that some of the faster ones (such as
Levenberg-Marquardt, which is part of the Optimization Toolbox) have
trouble converging on an answer with the noisy data the we have.
Thus, we have used the slowest but most stable routine provided by
MATLAB, \verb|fmins|.

Second, even the fastest routines (assuming they were stable) are not
terribly fast.  It could be that judicious tweaking of the "initial
guess" parameters; or use of analytic gradients of Eq. \ref{eq:fiteqn}
with respect to $\alpha$, $\beta$, and $\gamma$; or experimenting with
the different options the Optimization Toolbox provides could make
matters acceptable.  However, a general-purpose C routine would almost
certainly be much faster than anything written in MATLAB's interpreted
language.

Third, it would be very useful to have a general-purpose,
high-performance non-linear fitting routine for use within MATLAB for
{\em any} purpose, not just blood delay correction.

Unfortunately, the problem of finding a general-purpose, high-
performance fitting routine is fairly non-trivial; actually using a
general-purpose routine has turned out to be quite a hurdle.  I have
found two routines which might possibly fill our needs.  The first,
\verb|conmin| (constrained minimisation), was originally written in
FORTRAN, and I translated it to C using \verb|f2c|.  After some work,
the result was fairly readable C code (but with some lingering
aftertaste of FORTRAN, most notably a lot of \verb|goto|'s resulting
in hard-to-follow spaghetti code).  However, when I also translated
the simple example program to C and ran it, \verb|conmin| failed to
find a minimum; when I compiled and ran the original FORTRAN, it
worked.  Therefore, it seems that there was some difficult to find
problem introduced into the code by the FORTRAN to C conversion.

Next, I tried \verb|lmdif|, a Levenberg-Marquardt implementation from
the CEPHES package of C numerical routines.  This too was originally
written in FORTRAN (as part of the MINPACK project), but translated
into C by the author of CEPHES.  The translation was much more
complete than my quick work on \verb|conmin|, and the example program
supplied with \verb|lmdif| worked.  \verb|lmdif| works by calling
a user-supplied routine called \verb|fcn|, which takes as input a
vector $x$ and returns a vector $fvec$.  $x$ in this case has three
elements ($\alpha$, $\beta$, and $\gamma$) and $fvec$ has 12 (for the
study I was working with as a test, this is the number of frames in
the first 30 seconds).

\verb|lmdif| then finds the value of $x$ that minimises the sum-of-squares of
$fvec$.  So, I wrote a function \verb|fcn| that calls the C version of
\verb|b_curve| and finds the difference between $A(t)$ and the 12 values
returned by \verb|b_curve| (one for each frame).  Then, I wrote a
program that sets up all the parameters to \verb|lmdif| much the same
as the \verb|lmdif| example program did, calls \verb|lmdif|, and
displays the results.  After a couple of attempts, it became apparent
that \verb|lmdif| was taking the initial $x$, exploring a few dozen
other possibilities, and eventually returned the same $x$ that was
passed initially.

In an effort to get anything meaningful out of \verb|lmdif|, I then
wrote a very simple \verb|fcn|, that takes a single value $x$, and
returns a single value $fvec = x^{2}$.  \verb|lmdif| did in fact come
close to $x=0$ as a minimum of the function quickly.  However, it did
not stop -- it got as low as $x$ $10^{-60}$ before giving up after
performing too many calls to \verb|fcn| (the maximum number of calls
allowed by my main program was 400, on the recommendation of the
\verb|lmdif| example code).

In conclusion, then, it is clear that making use of even a finished,
well-polished least-squares fitting routine entails considerably more
than just writing a function to minimise and plugging all the right
parameters in.  This is definitely a project worthy of more time, but
such time is simply not available to us right now.

\end{document}
